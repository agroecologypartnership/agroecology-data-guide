[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Agroecology Partnership: Data Management Guidelines",
    "section": "",
    "text": "Introduction\nWelcome to the Data Management Guidelines documentation of the Agroecology Partnership. This online documentation expands on the official Agroecology Partnership Data Management Plan (DMP) and complements it with more practical and technically detailed information.\nWhile the DMP outlines the full strategy for data handling across the partnership and it is updated every 2 years, the present documentation is meant to be updated as we learn from doing. Its objective is to support partners with hands-on guidance, concrete examples, and step-by-step explanations of the data pipeline — from collection to publication — including how to make data FAIR (Findable, Accessible, Interoperable, and Reusable). This space aims to be more to the point than the DMP, helping teams implement the plan more effectively in day-to-day work. Furthermore, being a public document, we aim to offer the wider agronomy community with state-of-the-art data management practices in the Agroecology domain.\nFor any questions, please write us a line at agroecology-data@lifewatch.eu.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01-identify-data.html",
    "href": "01-identify-data.html",
    "title": "1  Identifying a data asset or research output",
    "section": "",
    "text": "Some project outputs are only useful internally, while others are relevant to people outside the project. The rule is:\n\nIf an output could be useful to someone outside the project, it is a research output and must be published — unless there is a valid reason not to.\n\nOutputs that are only useful within the project (e.g. working drafts, internal consultations, or coordination notes) should be stored only on the project’s internal cloud.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Identifying a data asset or research output</span>"
    ]
  },
  {
    "objectID": "02-submit-metadata.html",
    "href": "02-submit-metadata.html",
    "title": "2  Collect and send the metadata",
    "section": "",
    "text": "Metadata are data about data. They provide the essential context that allows us to understand, locate, and reuse a dataset or research output. Without metadata, information can easily become unusable or forgotten. With metadata, we ensure that each data asset is described clearly, remains findable, and can be connected to the broader research landscape.\nIn our partnership, collecting and submitting metadata is a collective responsibility. Whenever you identify a research output, you should first inform your task leader. The task leader then notifies the data team at agroecology-data@lifewatch.eu, while copying the Work Package lead. This way, every new output is tracked from the beginning.\nThe task leader is also responsible for collecting the metadata fields listed in Annex I of the DMP and included here below in the Table 1 for convenience. There are two ways to provide them:\n\nBy filling in the Excel metadata form available on our SharePoint.\nBy sending the necessary information directly to the data management team, who can complete the form on your behalf.\n\nOnce the metadata are collected, the data team will work with you to identify the type of data, select an appropriate standard, and decide on a trusted repository where the asset should be published. This ensures that our outputs are stored securely, remain discoverable, and align with FAIR principles.\n\nTable 1. Metadata fields and definitions. They are largely based on the DataCite schema, which is an international standard for describing research data. Each field has a short definition and an example to help you fill it in correctly.\n\n\n\n\n\n\n\n\n\nSection\nField\nDefinition\nExample\n\n\n\n\nData denomination\nData set reference name*\nA unique reference name starting with the prefix “DA_AGROECOLOGY”.\nRO_AGROECOLOGY_stakeholders_survey\n\n\n\nData set title*\nA short, descriptive title, easily searchable.\nStakeholders_Survey_Q2_2023\n\n\n\nDescription\nA brief explanation of the data content.\nSurvey on technological tools used in agroecology.\n\n\n\nKeywords\nTerms that describe the content and make it discoverable.\nsurvey, tools, living labs\n\n\n\nVersion number*\nVersion identifier for tracking changes.\nV 1.0\n\n\nData origin\nCreator(s)*\nNames, affiliations, and countries of dataset creators.\nJane Doe, University of X, Belgium\n\n\n\nData source\nHow or why the dataset was generated or re-used.\nGenerated dataset\n\n\n\nCreation date*\nDate when the dataset was produced.\n12.06.2023\n\n\n\nQuality assurance\nDescription of quality checks or validation performed.\nResponse rate monitoring, data profiling\n\n\nData specifications\nType*\nGeneral type of data.\nSurvey data\n\n\n\nFormat*\nFile format(s).\n.csv\n\n\n\nExpected size*\nApproximate dataset size.\n1 MB\n\n\nData accessibility\nData location*\nRepository where the dataset is stored.\nGBIF\n\n\n\nRepository submission date*\nDate of deposit.\n2023.07.15\n\n\n\nPersistent identifier (PID)*\nDOI or other permanent identifier.\nhttps://doi.org/10.xxxx/abcd\n\n\n\nAccess status*\nLevel of access (open, consortium, restricted).\nConsortium\n\n\n\nEmbargo period*\nEmbargo duration, if applicable.\nNo embargo\n\n\n\nFunding statement\nFunding acknowledgement text.\nThis dataset was generated within the AGROECOLOGY partnership funded by the EU.\n\n\nData utility\nSignificance inside the partnership*\nWork Package or task the dataset comes from.\nWP5 – Task 5.1\n\n\n\nSignificance outside the partnership\nWho else may benefit from the dataset.\nPolicy makers\n\n\nData publications\nRelated publications\nReferences to outputs derived from the dataset.\nDeliverable 3.2, Article XYZ\n\n\n\nData citation\nA ready-to-use citation for the dataset.\nDoe, J. & Smith, A. (2023). Stakeholders Survey Q2 2023. Zenodo.\n\n\n\n* Mandatory fields",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collect and send the metadata</span>"
    ]
  },
  {
    "objectID": "03-deposit-sharepoint.html",
    "href": "03-deposit-sharepoint.html",
    "title": "3  Deposit the data in the project’s internal repository",
    "section": "",
    "text": "The Agroecology partnership uses a dedicated SharePoint cloud drive as its internal repository. Access is restricted to project partners, and it serves as the central place to store and share data, analysis, and outputs. Whenever possible, you should save your work directly here, in the relevant directory, so that it remains available to the consortium and properly backed up.\nThe only exception concerns personal data. To comply with GDPR, any files containing personal information must not be uploaded to the shared drive. Instead, they must remain in your institution’s own secure storage.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deposit the data in the project's internal repository</span>"
    ]
  },
  {
    "objectID": "04-data-standarization.html",
    "href": "04-data-standarization.html",
    "title": "Data Standarization",
    "section": "",
    "text": "Data on Agroecology are multidisciplinary: There are biological data, socio-economic, geographical and many more. The main goal is to standardize data to the preferred community standard. However, sometimes there is not a clear winning standard, or the winning standard may not meet the FAIR principles. This chapter describes the preferred data standards for each data type collected in the Agroecology Parnership. In some cases, when there is not an obvious standard, we suggest evidence-based way of structuring data that fits the FAIR principles.\n\nSource: xkcd",
    "crumbs": [
      "Data Standarization"
    ]
  },
  {
    "objectID": "standarization-tabular-data.html",
    "href": "standarization-tabular-data.html",
    "title": "4  Tabular data",
    "section": "",
    "text": "4.1 Best practices for Tabular Data\nTabular data are often referred as data structured in rows and columns, where each row contains values for a set of properties and each column represents a specific property of the things described by the rows (Tennison and Kellogg 2015). Following certain best practices and well-studied structures when creating tabular data make it easier to analyze, visualize and reuse - by others or by your future self.\nTabular data can follow very different structures. There is a lot of flexibility in how you name your columns or what values you put in every cell. This flexibility is at the same time a double-sided knife: follow the right recommendations, and your data will be easy to analyze. Do the wrong decisions, and your same data could become not usable. Best practices go around columns names and consistency. Here we will describe common mistakes and how to solve them:",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular data</span>"
    ]
  },
  {
    "objectID": "standarization-tabular-data.html#best-practices-for-tabular-data",
    "href": "standarization-tabular-data.html#best-practices-for-tabular-data",
    "title": "4  Tabular data",
    "section": "",
    "text": "4.1.1 File formats\nWhile you are free to use proprietary spreadsheet software such as Microsoft Excel or Google Sheets to work on your data, we strongly recommend storing datasets in .csv or tab-delimited formats. This ensures compatibility across software and, most importantly, protects your work from accidental obsolescence. Imagine you open an old project and find that, after a software update, some formulas no longer work, column types have shifted, or the file won’t even open.\nOpen formats like .csv are readable by virtually any tool, now and decades from now. If a problem ever arises, there’s a global community of developers who can build solutions to recover your data. Proprietary formats are fine for analysis, but their internal rules can change without notice, and you might only discover it when it’s too late.\nIn short: use Excel or Sheets to crunch numbers if you like, but keep your raw data in .csv and publish both raw and processed datasets in .csv whenever possible. Avoid .xlsx as your master copy. And you can always open .csv in Excel for analysis without locking your work into a single program’s rules.\n\n\n4.1.2 Handling Empty cells\nMost software handling text, tabular files, are able to recognize empty cells. This is, a cell where a value is not added. In Excel, these show as blanks, while in other tools like R or Python they show as “NA” or “NULL”. We recommend to leave empty cells when a value is not known. A common mistake is to add 0’s in a numeric variable when a value is unknown. This could distort further analysis as 0’s are considered meaningful values. Common file formats that allow empty values are .csv, .txt or .xlsx.\nCertain formats don’t allow adding empty values. In this case, it is recommended to add a placeholder such as -9999 in numeric variables, or NA in text variables. This blank value must be documented properly. For instance, the binary format NetCDF does not allow empty cells, but you can document empty values by using the attributes missing_value or _FillValue.\n\n\n4.1.3 Column names\nThe names used to describe your variables should be clear and consistent. Avoid spaces or special characters, and adopt standard units. Examples:\n\nmax_temp_celsius instead of Max Temp Celsius\nsoil_ph instead of Soil-pH\n\nWe recommend to use lowercase and underscores _ instead of spaces. This style is known as snake_case. Other well-known style is camelCase, where spaces are avoided and words are spitted by using a Capital letter to emphasize the different words.\n\n\n4.1.4 Adhere to community standards\nThink of data like a written language: if everyone makes up their own grammar, nobody can understand each other. In data, this applies not only between humans but also between humans and machines. We rely on computers to analyze our data, but unlike us, they’re terrible at guessing meaning. To them, data are just zeros and ones. In science, the key is in the context, so we need to shape our data so both people and machines can read them correctly.\nThe easiest way to do that? Follow community standards. Chances are, you’re not the first person working in your field. Others have faced the same problems and worked hard to solve them. Communities dealing with certain types of data — geospatial, surveys, biodiversity, genetics — have already agreed on rules and formats so that other researchers and software can make sense of the data. Take dates as an example. You could write them in many ways: DD/MM/YYYY, MM/DD/YYYY, or even with month names. But most software understands them best in the format YYYY-MM-DD. This is not arbitrarty: it’s an international standard (ISO 8601) designed after much discussion and testing.\nIn the next chapters, we will look at several community standards relevant to Agroecology, and we will recommend which ones to use for maximum clarity and interoperability.\n\n\n4.1.5 Separate raw data from calculations\nRaw data should always live apart from any data cleaning, wrangling, or analysis. Even if your raw file has typos, strange characters, or awkward formatting — leave them as they are. They’re part of the original record.\nDo all cleaning and analysis in a separate file or script (R, Python, Julia, Excel formulas…) and share that too. This will enable anyone can reproduce your process and get the same results. This is of course, unless you are unsure of your analysis and embarrassed to show to others. In that case you should proofread until you are happy.\nWhy keep the artifacts? Because it preserves a trustworthy copy of the original, makes your work reproducible, and gives you a safe point to return to if something later goes wrong.",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular data</span>"
    ]
  },
  {
    "objectID": "standarization-tabular-data.html#table-schema",
    "href": "standarization-tabular-data.html#table-schema",
    "title": "4  Tabular data",
    "section": "4.2 Table schema",
    "text": "4.2 Table schema\nA table schema is a data dictionary that standardizes and explains each variable in your dataset. It records key information such as the data type of each column (numeric, date, string, boolean…), expected units, allowed values, or format rules. By clearly describing what each column should contain, the schema ensures that your data is consistent, interpretable, and aligned with project standards.\nThe schema is stored as a separate file that should accompany your dataset. It is typically named after the dataset it describes, for example: mydataset.schema.json, mydataset.schema.yaml, or even a text/Word version for non-technical users. Including the schema with the dataset provides a clear reference for anyone using or reviewing the data.\nA schema also allows datasets to enforce rules automatically: for instance, specifying which fields are required, the expected data type for each column, the valid range of numeric values, or the expected format for dates and text. These rules make the dataset self-descriptive and easier to integrate with other datasets following the same standards.\n\n4.2.1 Frictionless Table Schema\nWithin the Agroecology Partnership, we recommend using the Frictionless Table Schema to define and document tabular datasets. Frictionless is an open-source framework designed to reduce common data workflow issues—what they call friction. It provides both standards and software: the standards define how data should be structured, and the software applies these rules to automatically transform, validate, and describe datasets.\nIn Agroecology, we will use one of the Frictionless standards: The Frictionless Table Schema. This specification allows you to define tabular data—such as field names, data types and formats. While it is primarily designed for text-based tables like CSV, it can be extended to other tabular formats.\nFor example, consider a minimal dataset with three columns: crop_species, crop_yield_kg_per_ha, and planting_date. The dataset contain inconsistencies such as empty cells, inconsistent numeric formats, or irregular date formats. We will see how to detect this issues automatically in the next section Quality Control.\n\n\n\ncrop_species\ncrop_yield_kg_per_ha\nplanting_date\n\n\n\n\nMaize\n5200\n2023-03-15\n\n\nWheat\n4.8 tons\n2023-03-22\n\n\nmaiz\n4900\n\n\n\nRice\n\n2023-04-01\n\n\nBarley\n5500\n2023/03/18\n\n\n\nA table schema describes each column with properties like:\n\nname: the column name\ntype: expected data type (string, number, date)\ndescription: human-readable explanation of the field\nformat: expected format for the values\nrdfType: optional semantic identifier linking the field to a concept in a controlled vocabulary (See the note below for more information)\n\nApplied to our example:\nfields:\n  - name: crop_species\n    type: string\n    description: Crop species (scientific name)\n    rdfType: http://aims.fao.org/aos/agrovoc/c_1972\n  - name: crop_yield_kg_per_ha\n    type: number\n    description: Crop yield in kilograms per hectare\n    rdfType: http://aims.fao.org/aos/agrovoc/c_10176\n  - name: planting_date\n    type: date\n    format: '%Y-%m-%d'\n    description: Date when the crop was planted\n    rdfType: http://aims.fao.org/aos/agrovoc/c_24065\nprimaryKey: crop_species\nmissingValues:\n  - \"\"\nAll this information is saved into a file. The standard accepts two machine readable formats: json and yaml. For us however, we will also accept text formats such as MS Word. While this is a not machine readable format and not widely used for this purpose, we understand that is a well-known format by the general public, and can act as an entry point to encourage non-technical users to document their data. We consider that it’s better to have documentation in a human-readable format than none at all. Converting a bullet point list in Markdown or Word to JSON/YAML later is a minimal effort compared to the cost of having undocumented data. You can see this example in the three formats below by clicking in each of them\n\n\nexample.schema.json\n\n{\n  \"fields\": [\n    {\n      \"name\": \"crop_species\",\n      \"type\": \"string\",\n      \"description\": \"Crop species (scientific name)\",\n      \"rdfType\": \"http://aims.fao.org/aos/agrovoc/c_1972\"\n    },\n    {\n      \"name\": \"crop_yield_kg_per_ha\",\n      \"type\": \"number\",\n      \"description\": \"Crop yield in kilograms per hectare\",\n      \"rdfType\": \"http://aims.fao.org/aos/agrovoc/c_10176\"\n    },\n    {\n      \"name\": \"planting_date\",\n      \"type\": \"date\",\n      \"description\": \"Date when the crop was planted (format: yyyy-MM-dd)\",\n      \"format\": \"yyyy-MM-dd\",\n      \"rdfType\": \"http://aims.fao.org/aos/agrovoc/c_24065\"\n    }\n  ],\n  \"primaryKey\": \"crop_species\",\n  \"missingValues\": [\"\"]\n}\n\n\n\nexample.schema.yaml\n\nfields:\n  - name: crop_species\n    type: string\n    description: Crop species (scientific name)\n    rdfType: http://aims.fao.org/aos/agrovoc/c_1972  # crops\n  - name: crop_yield_kg_per_ha\n    type: number\n    description: Crop yield in kilograms per hectare\n    rdfType: http://aims.fao.org/aos/agrovoc/c_10176  # crop yield\n  - name: planting_date\n    type: date\n    format: '%Y-%m-%d'\n    description: Date when the crop was planted\n    rdfType: http://aims.fao.org/aos/agrovoc/c_24065  # planting date\nprimaryKey: crop_species\nmissingValues:\n  - \"\"\n\nexample.schema.docx\n\n\n\n\n\n\nNote\n\n\n\nThe property rdfType is an optional property we use to link each field to a concept in a controlled vocabulary or ontology. It helps machines (and humans) understand the meaning of a field beyond its name and link to many other resources in the web, and plays an important role in building AI-ready, machine-readable datasets and knowledge graphs.\nFor example, the AGROVOC multilingual thesaurus by FAO has a controlled vocabulary for the word “crop yield”, linked to the same term in many languages and linkes to other concepts. It can be found at the URI below:\nhttp://aims.fao.org/aos/agrovoc/c_10176\nOther ontologies with controlled vocabularies that are useful in Agroecology are the Ecoportal, the Agroportal or the Survey Ontology.",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular data</span>"
    ]
  },
  {
    "objectID": "standarization-tabular-data.html#tidy-data-for-seamless-analysis",
    "href": "standarization-tabular-data.html#tidy-data-for-seamless-analysis",
    "title": "4  Tabular data",
    "section": "4.3 Tidy data for seamless analysis",
    "text": "4.3 Tidy data for seamless analysis\nWe recommend to follow a wide tabular data structure for archiving and publishing tabular data, as described before. This is because the table schema allows to explain in detail the data, and quality control can be automatized.\nHowever, when analyzing these data, following the principles of tidy data can make tabular datasets easier to work with (Wickham 2014). In tidy data, each row represents a single observation, each column a variable, and each table a distinct type of observation. For example, consider this table with grape yield and wine production across three European regions . A non-tidy table might look like this:\n\n\n\nfarm\ngrape_yield_kg_h\nwine_production_hl_ha\n\n\n\n\nBavaria (DE)\n7000\n60\n\n\nLa Rioja (ES)\n5000\n40\n\n\nPeloponnese (GR)\n6000\n50\n\n\n\nHere, different variables are spread across columns, and units are written inside the values. While this structure is easy to understand for most of us, it is harder to analyze for machines:\n\nVariable names as column headers: Each variable requires a separate column, so adding new variables (e.g., sugar content, harvest date) would require adding new columns, breaking automation.\nInconsistent row structure: If some observations are missing a value, it’s harder to handle missing data systematically.\nLimited scalability: Combining multiple datasets or reshaping data for plotting, statistical analysis, or modeling requires extra preprocessing steps.\nFiltering and grouping complexity: Aggregating or comparing values across variables requires multiple steps instead of simple column-based operations.\n\nA tidy version of the same data represents each measurement as a separate row with explicit columns for the variable type, value, and unit, so that each row contains a single observation of a single variable.\n\n\n\nFarm\nVariable\nValue\nUnit\n\n\n\n\nBavaria (DE)\nGrape Yield\n7000\nkg/h\n\n\nBavaria (DE)\nWine Production\n60\nhl/ha\n\n\nLa Rioja (ES)\nGrape Yield\n5000\nkg/h\n\n\nLa Rioja (ES)\nWine Production\n40\nhl/ha\n\n\nPeloponnese (GR)\nGrape Yield\n6000\nkg/h\n\n\nPeloponnese (GR)\nWine Production\n50\nhl/ha\n\n\n\nThis tidy format makes it easy to filter, group, or plot by crop type, farm, or yield, and it aligns with modern data analysis workflows. Following these principles helps ensure that tabular data are both clear and practical for downstream use.\n\n\n\n\nTennison, Jeni, and Gregg Kellogg. 2015. “Model for Tabular Data and Metadata on the Web.” World Wide Web Consortium; W3C Recommendation. https://www.w3.org/TR/2015/REC-tabular-data-model-20151217/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular data</span>"
    ]
  },
  {
    "objectID": "standarization-survey-data.html",
    "href": "standarization-survey-data.html",
    "title": "5  Survey data",
    "section": "",
    "text": "5.1 Survey project structure\nRecommendations for organizing survey data in the Agroecology partnership following a wide, spreadsheet format approach. This structure is inspired by the data structure of the American National Election Studies (ANES 2020), the recommendations of (Zimmer, Powell, and Velásquez 2024), while applying a tidy data approach (Wickham 2014) with usability on mind and the enabling of a later transformation into Open Linked Data as structured in The Survey Ontology (Scrocca et al. 2021).\nWe propose a project structure that uses csv files as a way of formally describing the survey, both questions, answer and other information. This structure may be accompanied of a text description of the survey for further context. A minimal example of the file directory would look like:\nThe data files are inside the data directory, and the different documents including the description of the survey are inside the docs directory.\nThis structure can be further extended, depending on the needs of the survey. For instance, a README file can be added to the root of the project with a short explanation of the survey. When uploading to Zenodo, this can be written instead in the Description of the metadata. If the survey is uploaded to GitHub, we recommend to use the markdown format for the README file. Also on GitHub, it is recommended to include a LICENSE file, choosing the license of the data.\nIn addition, any notebook or spreadsheet used to analyze the data can be added to the analysis folder, and reusable code scripts can be added to the scripts directory.",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "standarization-survey-data.html#survey-project-structure",
    "href": "standarization-survey-data.html#survey-project-structure",
    "title": "5  Survey data",
    "section": "",
    "text": ".\n├── 📁 data/\n│   ├── codebook.csv\n│   └── responses.csv\n└── 📁 docs/\n    └── survey_descriptor.pdf\n\n\n\n.\n├── README.md\n├── LICENSE.md\n├── 📁 data/\n│   ├── codebook.csv\n│   └── responses.csv\n├── 📁 docs/\n│   ├── survey_descriptor.pdf\n│   └── survey_analysis.pdf\n├── 📁 scripts/\n│   └── clean_data.R\n└── 📁 analysis/\n    └── survey_analysis.xlsx or survey_analysis.ipynb",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "standarization-survey-data.html#survey-project-description-file",
    "href": "standarization-survey-data.html#survey-project-description-file",
    "title": "5  Survey data",
    "section": "5.2 Survey project description file",
    "text": "5.2 Survey project description file\nA survey is always quite specific, and cannot be fully understood without a good explanation of what is the context and the rationale behind the different questions. It is also a sign of respect to the respondents to explain what is the goal of your survey. We propose to include this description in a survey_descriptor.pdf file in the docs/ directory.",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "standarization-survey-data.html#survey-project-data-files",
    "href": "standarization-survey-data.html#survey-project-data-files",
    "title": "5  Survey data",
    "section": "5.3 Survey project data files",
    "text": "5.3 Survey project data files\nSurveys are typically are typically stored as pdf, docx or xlsx. Having interoperability on mind, we propose to use text delimited files such as csv. These type of files are largely used in data science. they have several advantages, including easy machine-readability and being an open format with no owner, which ensures data will remain readable and understandable by many different software for a long time. We propose the following specifications:\n\nSeparate columns using semicolons ;\nUse double-quotes \" to quote strings\nUse UTF-8 as encoding.\n\nWe avoid using colons , as separators because these can be used in free text, open questions. They would affect the structure of the data. We recommend to prohibit the use of semicolons ; and double-quotes \" in any case that is not separating columns and delimiting strings respectively. We encourage you to use software solutions that allow blocking these characters in free-text answers provided by users. If blocking the use of these characters it is not possible, we urge you to ask respondents to not use them while filling up your survey.\n\n5.3.1 Codebook\nCodebooks are files that explain the questions formulated in the survey. An unique identifier (a code) is assigned to each question, linking the information about the questions with the answers provided by the participants. But the codebook can be used as well during the design of the survey.\nThe example below shows an hypothetical survey codebook about the user satisfaction using an online platform.\n./data/codebook.csv\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nLabel\nType\nQuestionText\nValues\nCardinality\nQuestionType\n\n\n\n\nQ1_age\nAge\ninteger\nWhat is your age?\n\n1..1\nSingleChoice\n\n\nQ2_gender\nGender\nstring\nWhat is your gender?\nFemale|Male|Other\n1..1\nSingleChoice\n\n\nQ3_satisfaction\nSatisfaction\ninteger\nHow satisfied are you with the platform?\n1=Very unsatisfied|2=Unsatisfied|3=Neutral|4=Satisfied|5=Very satisfied\n1..1\nSingleChoice\n\n\nQ4_improvement\nImprovement\nstring\nWhat would you improve in the platform?\n\n0..n\nFreeText\n\n\n\nEach row in the codebook describes a survey question. Below is an explanation of each column:\n\nCode: A unique identifier for the question. It must be unique within the survey.\nLabel: A short, human-readable label or name for the variable that can be used in spreadsheets or statistical software.\nType: The data type of the answer. Common types include “integer”, “string”, “boolean”, or “date”.\nQuestionText: The full text of the question as it was asked in the survey.\nValues: A list of possible values for closed questions. Options are separated by vertical bars (|), and value labels can be assigned using the equals sign (=). For open or free-text questions, this field is left empty.\nCardinality: Indicates how many answers are allowed. The cardinality pattern is min..max, where the first number is the minimum required answers and the second is the maximum allowed.  n denotes “no fixed upper limit,” so 1..1 means exactly one answer, while 0..n means the question may be skipped or answered multiple times.\nQuestionType: Describes the nature of the question. Typical values include “SingleChoice”, “MultipleChoice” or “FreeText”\n\n\n\n5.3.2 Responses\nAnswers to the survey are recorded in the ./data/responses.csv file. Every row is the answers of a participant, and every column is named after the code in codebook.csv. This allows to link easily the information about the questions without getting the responses file full of details that difficult the analysis.\n./data/responses.csv\n\n\n\n\n\n\n\n\n\n\nrespondent_id\nQ1_age\nQ2_gender\nQ3_satisfaction\nQ4_improvement\n\n\n\n\n001\n34\nfemale\n4\nI think the platform is user-friendly\n\n\n002\n29\nmale\n5\nNeeds better support for collaboration.\n\n\n\nWe recommend to include an unique identifier for every respondent, here named as respondent_id. This allows to annomymize the survey without loosing the link to private information about the respondents that might have been collected (e.g. name, email, address)\n\n\n5.3.3 Participants and private information\nSometimes your survey might collect personal or sensitive information — like names, emails, or locations. If that’s the case, you should never share this data publicly. In the Agroecology Partnership, we follow the GDPR (the General Data Protection Regulation), which means that private data must be handled carefully and securely.\nWe recommend creating a separate file (not to be published!) called participants.csv inside your data/ folder. This file is for internal use only — for example, if you need to follow up with participants or validate something later. To keep things tidy and safe, give each person a unique code, which we’ll call respondent_id and store personal info (like name or email) in separate columns. Use clear, human-friendly column names, and write them using snake_case (i.e. separate the words of the column names with underscores _).\n\n\n\n\n\n\n\n\n\n\n\nrespondent_id\nname\nemail\ncountry\npreferred_language\nwants_emails\n\n\n\n\n001\nJane Doe\njane.doe@example.com\nFrance\nfr\nFALSE\n\n\n002\nJohn Doe\njohn.doe@example.com\nUSA\nen\nTRUE\n\n\n\nThe table above is an example (with mocked information) of how the participats.csv would look like. This file helps you anonymize your actual survey data, because in the main responses.csv, you’ll only keep the respondent_id, and this is nothing that could personally identify someone.\n\n\n\n\nANES. 2020. “Time Series Study Full Release: User Guide and Codebook.” https://electionstudies.org/wp-content/uploads/2022/02/anes_timeseries_2020_userguidecodebook_20220210.pdf.\n\n\nScrocca, Marco, Daniele Scandolari, Giulia Re Calegari, Irene Baroni, and Irene Celino. 2021. “The Survey Ontology: Packaging Survey Research as Research Objects.” In Proceedings of the 2nd Workshop on Data and Research Objects Management for Linked Open Science – Co-Located with ISWC 2021. https://doi.org/10.4126/FRL01-006429412.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nZimmer, Samantha A., Ryan J. Powell, and Iván C. Velásquez. 2024. Exploring Complex Survey Data Analysis Using r: A Tidy Introduction with {Srvyr} and {Survey}. Chapman & Hall/CRC Press. https://tidy-survey-r.github.io/tidy-survey-book/.",
    "crumbs": [
      "Data Standarization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "05-quality-control.html",
    "href": "05-quality-control.html",
    "title": "6  Quality Control of Data",
    "section": "",
    "text": "6.1 Frictionless Data Packages and the Frictionless Table Schema\nValidating the data is about making sure data makes sense before sharing it. For tabular data, this means checking that required fields are not empty, that numbers are really numbers, and that dates or text follow a consistent format. Verification looks outward: values should also be plausible, like coordinates pointing to the right country or measurements staying within expected ranges.\nDoing this by hand is tedious and error-prone, which is why we rely on tools. A good way to start is to define an schema: a file that declares rules about the columns in your table. The schema establishes if the columns have numeric, character or date values. It sets expectations such as minimum and maximum values or mandatory fields.\nOnce the schema is in place, there are that tools can automatically check your dataset against it and flag any problems: missing values, numbers in the wrong format, typos in categories, or values outside the valid limits. This shifts the burden from manual inspection to automated validation, making the process faster, more consistent, and easier to repeat every time the dataset is updated.\nWithin the Agroecology Partnership, we recommend the use of the Frictionless Table Schema to validate and verify datasets automatically. In the section about standardizing tabular data, we explained how this standard allow to define the columns of your data table. Furthermore, the standard allows to write rules to automatically validate the data: what are the data types, expected values, maximum and minimum values, handle missing values, and many more.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quality Control of Data</span>"
    ]
  },
  {
    "objectID": "05-quality-control.html#frictionless-data-packages-and-the-frictionless-table-schema",
    "href": "05-quality-control.html#frictionless-data-packages-and-the-frictionless-table-schema",
    "title": "6  Quality Control of Data",
    "section": "",
    "text": "6.1.1 Validating Data with the Open Data Editor\nThe Open Data Editor is a user-friendly tool that leverages the Frictionless Table Schema to perform automated checks on datasets. The technical documentation of the Open Data Editor explains in detail how to install the app, and how to highlight errors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quality Control of Data</span>"
    ]
  },
  {
    "objectID": "06-deposit-repository.html",
    "href": "06-deposit-repository.html",
    "title": "7  Data Publishing",
    "section": "",
    "text": "Once the data is correctly formatted, data should be published in a trusted repository, ideally before 6 months since its creation and always before the end of the Agroecology partnership. You should archive your data in a trusted repository even if you are writing scientific publication based on them, but archiving does not mean the data will be public yet. Most repositories allow to submit data and keep it private until you decide. A list of trusted repositories is published by (Lazzeri 2024).\nOnce the data are in the trusted repository, you should get a DOI for your data. You can include the DOI in the spreadsheet in sharepoint, or mail agroecology-data@lifewatch.eu with the DOI and we will do it for you. If the repository does not provide a DOI, let us know.\n\n\n\n\nLazzeri, E. 2024. “Update of the Study on the Readiness of Research Data and Literature Repositories to Facilitate Compliance with the Open Science Horizon Europe MGA Requirements (1.0).” Zenodo. https://doi.org/10.5281/zenodo.13919643.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Publishing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "ANES. 2020. “Time Series Study Full Release: User Guide and\nCodebook.” https://electionstudies.org/wp-content/uploads/2022/02/anes_timeseries_2020_userguidecodebook_20220210.pdf.\n\n\nLazzeri, E. 2024. “Update of the Study on the Readiness of\nResearch Data and Literature Repositories to Facilitate Compliance with\nthe Open Science Horizon Europe MGA Requirements (1.0).” Zenodo.\nhttps://doi.org/10.5281/zenodo.13919643.\n\n\nScrocca, Marco, Daniele Scandolari, Giulia Re Calegari, Irene Baroni,\nand Irene Celino. 2021. “The Survey Ontology: Packaging Survey\nResearch as Research Objects.” In Proceedings of the 2nd\nWorkshop on Data and Research Objects Management for Linked Open Science\n– Co-Located with ISWC 2021. https://doi.org/10.4126/FRL01-006429412.\n\n\nTennison, Jeni, and Gregg Kellogg. 2015. “Model for Tabular Data\nand Metadata on the Web.” World Wide Web Consortium; W3C\nRecommendation. https://www.w3.org/TR/2015/REC-tabular-data-model-20151217/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nZimmer, Samantha A., Ryan J. Powell, and Iván C. Velásquez. 2024.\nExploring Complex Survey Data Analysis Using r: A Tidy Introduction\nwith {Srvyr} and {Survey}. Chapman & Hall/CRC Press. https://tidy-survey-r.github.io/tidy-survey-book/.",
    "crumbs": [
      "References"
    ]
  }
]